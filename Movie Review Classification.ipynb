{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'than', 'are', 'his', 'ain', 're', 'below', 'myself', 'herself', 'about', 'there', 'so', 'had', 'hasn', 'by', 'her', 'too', 'doesn', 'further', 'few', 'against', 'from', 'hers', 'just', \"wasn't\", \"don't\", 'itself', 'can', 'how', 'why', 'mustn', 'o', \"couldn't\", 'or', 'other', 'does', 'couldn', 'into', 'same', 'over', 'is', \"shan't\", 'doing', \"should've\", 'd', 'that', 'were', 'hadn', 'both', 'as', 'no', 'wasn', \"that'll\", 'with', 'mightn', \"mustn't\", 'ours', \"didn't\", 'those', 'ourselves', \"aren't\", 'on', 'themselves', 'the', 'where', 'now', 'out', 'nor', 'have', 'again', 'until', 'an', 'needn', 'himself', 'yourselves', 'our', 'they', 'in', 'should', 'you', \"needn't\", \"isn't\", 'isn', 'will', 'them', 'but', 'once', 'not', 'what', 'has', \"she's\", 'be', 'before', 'during', 'above', 'theirs', 'me', 'she', 'your', 'between', 'more', \"you'll\", 'if', 'been', 'which', 'after', \"haven't\", 'i', 'own', 'down', 'don', \"hadn't\", 'through', 'and', 'under', 'when', 'shan', 'we', 'only', 'some', 'll', 'to', 'each', 'did', 'up', 'most', 'wouldn', 'shouldn', 'him', \"won't\", 'because', \"wouldn't\", 'aren', \"doesn't\", 'of', 'it', 'all', 'any', 'this', 'y', 'won', 'weren', 'haven', 'a', \"hasn't\", \"you'd\", 't', \"shouldn't\", 'who', 'at', 'was', 'its', 'such', \"you're\", 'didn', 'while', 've', 'yourself', \"you've\", 'yours', 'for', \"it's\", 'my', 'he', 'am', 'whom', 'being', 'do', 's', 'very', 'these', \"weren't\", \"mightn't\", 'ma', 'then', 'having', 'm', 'off', 'their', 'here'}\n"
     ]
    }
   ],
   "source": [
    "# Initializing objects\n",
    "tokenizer=RegexpTokenizer(r'\\w+') # To extract all the words\n",
    "ps=PorterStemmer()\n",
    "en_stopwords=set(stopwords.words('english'))\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['than', 'are', 'his', 'ain', 're', 'below', 'myself', 'herself', 'about', 'there', 'so', 'had', 'hasn', 'by', 'her', 'too', 'doesn', 'further', 'few', 'against', 'from', 'hers', 'just', 'itself', 'can', 'how', 'why', 'mustn', 'o', 'or', 'other', 'does', 'couldn', 'into', 'same', 'over', 'is', 'doing', \"should've\", 'd', 'that', 'were', 'hadn', 'both', 'as', 'no', 'wasn', \"that'll\", 'with', 'mightn', 'ours', 'those', 'ourselves', \"aren't\", 'on', 'themselves', 'the', 'where', 'now', 'out', 'have', 'again', 'until', 'an', 'needn', 'himself', 'yourselves', 'our', 'they', 'in', 'should', 'you', \"needn't\", \"isn't\", 'isn', 'will', 'them', 'but', 'once', 'what', 'has', \"she's\", 'be', 'before', 'during', 'above', 'theirs', 'me', 'she', 'your', 'between', 'more', \"you'll\", 'if', 'been', 'which', 'after', 'i', 'own', 'down', 'don', 'through', 'and', 'under', 'when', 'shan', 'we', 'only', 'some', 'll', 'to', 'each', 'did', 'up', 'most', 'wouldn', 'shouldn', 'him', 'because', \"wouldn't\", 'aren', \"doesn't\", 'of', 'it', 'all', 'any', 'this', 'y', 'won', 'weren', 'haven', 'a', \"you'd\", 't', 'who', 'at', 'was', 'its', 'such', \"you're\", 'didn', 'while', 've', 'yourself', \"you've\", 'yours', 'for', \"it's\", 'my', 'he', 'am', 'whom', 'being', 'do', 's', 'very', 'these', 'ma', 'then', 'having', 'm', 'off', 'their', 'here']\n"
     ]
    }
   ],
   "source": [
    "# Removing negative words from stopwords\n",
    "negative_words=[\"not\", \"won't\", \"wasn't\", \"weren't\", \"nor\", \"neither\", \"mustn't\", \"mightn't\", \"shouldn't\", \"haven't\", \"hasn't\", \"hadn't\", \"don't\", \"didn't\", \"shan't\", \"couldn't\"] \n",
    "en_stopwords=[word for word in en_stopwords if word not in negative_words]\n",
    "print(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedReview(review):\n",
    "    review=review.lower()\n",
    "    review=review.replace('<br/>', ' ')\n",
    "    # Tokenize\n",
    "    tokens=tokenizer.tokenize(review)\n",
    "    filtered_tokens=[token for token in tokens if token not in en_stopwords]\n",
    "    stemmed_tokens=[ps.stem(token) for token in filtered_tokens]\n",
    "    stemmed_review=' '.join(stemmed_tokens)\n",
    "    return stemmed_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStemmedDocument(outputFile, inputFile):\n",
    "    out=open(outputFile, 'w', encoding='utf-8')\n",
    "    with open(inputFile, encoding='utf-8') as f:\n",
    "        reviews=f.readlines()\n",
    "    for review in reviews:\n",
    "        stemmed_review=getStemmedReview(review)\n",
    "        print(stemmed_review, file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=['This was an awesome movie.',\n",
    "  'Great movie! I liked it a lot.',\n",
    "   'Bad, not upto the mark.',\n",
    "   'Happy ending. Awesome acting by the hero.',\n",
    "   'Surely a disappointing movie',\n",
    "   'Loved it! Truely great.',\n",
    "   'Could have been better.'\n",
    "  ]\n",
    "Y=[1, 1, 0, 1, 0, 1, 0] # 1-Positive 0-Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt=['I was happy and I loved the acting in the movie',\n",
    "   'The movie that I saw was not bad.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning\n",
    "X_clean=[getStemmedReview(i) for i in X]\n",
    "Xt_clean=[getStemmedReview(i) for i in Xt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesom movi',\n",
       " 'great movi like lot',\n",
       " 'bad not upto mark',\n",
       " 'happi end awesom act hero',\n",
       " 'sure disappoint movi',\n",
       " 'love trueli great',\n",
       " 'could better']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happi love act movi', 'movi saw not bad']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1]\n",
      " [1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "['act', 'awesom', 'bad', 'better', 'could', 'disappoint', 'end', 'great', 'happi', 'hero', 'like', 'lot', 'love', 'mark', 'movi', 'not', 'sure', 'trueli', 'upto']\n"
     ]
    }
   ],
   "source": [
    "X_vec=cv.fit_transform(X_clean).toarray()\n",
    "print(X_vec)\n",
    "print(cv.get_feature_names()) # Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Xt_vec=cv.transform(Xt_clean).toarray()\n",
    "print(Xt_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.fit(X_vec, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(Xt_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10761126, 0.89238874],\n",
       "       [0.766035  , 0.233965  ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posterior probalilities\n",
    "mnb.predict_proba(Xt_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multivariate berouli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb=BernoulliNB(binarize=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb.fit(X_vec, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.predict(Xt_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10761126, 0.89238874],\n",
       "       [0.766035  , 0.233965  ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Posterior probalilities\n",
    "mnb.predict_proba(Xt_vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
